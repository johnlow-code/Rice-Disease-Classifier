{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'><font color=\"orange\">The Rice is Right:</font></h1>\n",
    "<h1 align='center'>Early Detection of Rice Leaf Disease using Convolutional Neural Networks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highlights\n",
    "\n",
    "In the Philippines, rice farming industry is very dominant encompassing over 10 million Filipinos. However, these families earn as low as 10 pesos due to the current economy in the industry. Add this to the fact that 37\\% of their production are lost due to pests and diseases.\n",
    "\n",
    "This study aims to help farmers by early detection of disease through rice leaf image processing using convolutional neural networks. The model is trained on the Rice Disease Image Dataset by Huy Minh Do which includes over 3000 rice leaf images. Over 4 categories: Leaf Blast, Brown Spot, Hispa, and Healthy; the model performs well at 78.2\\% accuracy. We also looked at how it performs on classifying a healthy rice leaf and found that the model performed even better at 92.2\\%. Furthermore, we saw that the model is capable even for cases where the symptoms of the disease are still very small. This model gives farmers ample time to potentially save their crops, have better yield, and save cost from fertilizers and pesticides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used for this project is [Rice Diseases Image Dataset](https://www.kaggle.com/minhhuy2810/rice-diseases-image-dataset/version/1?fbclid=IwAR3QwbQzFpHLe_KCelIbrrMB4kwaBfhzJhrcqLwX7DEOJmLfkI4ZRF2le4U) compiled by Huy Minh Do and uploaded in Kaggle. This consists of 3355 images of isolated rice leaves with four classifications: Healthy (1488) or afflicted with one of the diseases: Hispa (565), Brown Spot (523), or Leaf Blast (779). The image sizes range from 734 by 734 pixels to 3120 by 3120 pixels. Sample images of the dataset for each classification is shown in Figure 1. \n",
    "\n",
    "<img src='sampleimages.png'>\n",
    "<div align='center'>Figure 1. Sample images of rice leaves per classification: Leaf Blast, Brown Spot, Hispa, and Healthy.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "Before any modeling or classification is performed on the images, several image pre-processing steps must be performed first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape\n",
    "\n",
    "To account for any limitations that may arise due to camera specifications, all images are reshaped to 300 by 300 pixels, regardless of initial shape. In doing so, the model that will be trained in the succeeding parts will be applicable to even low-quality images. We use PIL for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T08:56:13.064463Z",
     "start_time": "2019-12-23T08:56:13.036462Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = 'D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T14:04:35.575257Z",
     "start_time": "2019-12-11T14:04:35.013398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset/Labelled\\\\BrownSpot',\n",
       " 'D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset/Labelled\\\\Healthy',\n",
       " 'D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset/Labelled\\\\Hispa',\n",
       " 'D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset/Labelled\\\\LeafBlast']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_type = glob.glob(main_directory + '/Labelled/*')\n",
    "files = glob.glob(main_directory + '/Labelled/*/*')\n",
    "labelled_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T14:04:59.310867Z",
     "start_time": "2019-12-11T14:04:59.305905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset/Resized\\\\BrownSpot',\n",
       " 'D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset/Resized\\\\Healthy',\n",
       " 'D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset/Resized\\\\Hispa',\n",
       " 'D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset/Resized\\\\LeafBlast']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape_type =  list(map(lambda x: x.replace('/Labelled\\\\', '/Resized\\\\'), labelled_type))\n",
    "files_reshape = list(map(lambda x: x.replace('/Labelled\\\\', '/Resized\\\\'), files))\n",
    "reshape_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make resize path\n",
    "for t in reshape_type:\n",
    "    try:\n",
    "        os.makedirs(t)\n",
    "    except FileExistsError:\n",
    "        print(\"File \", t, 'exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T14:13:12.364526Z",
     "start_time": "2019-12-11T14:05:12.341279Z"
    }
   },
   "outputs": [],
   "source": [
    "basewidth = 224\n",
    "for file, file_save in zip(files, files_reshape):\n",
    "    img = Image.open(file)\n",
    "    wpercent = (basewidth/float(img.size[0]))\n",
    "    hsize = int((float(img.size[1])*float(wpercent)))\n",
    "    img = img.resize((basewidth,basewidth), Image.ANTIALIAS)\n",
    "    img.save(file_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To do this, a mask is created using the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T10:48:17.242076Z",
     "start_time": "2019-12-11T10:48:17.234081Z"
    }
   },
   "source": [
    "### Background removal\n",
    "After reshaping, the foreground of the image, i.e. the actual leaf, must be separated from the background to reduce noise and data leakage, which would then lead to easier model training. With this, we need to do automated background removal such that an actual photo not taken in an isolated environment can be classified properly by our model.\n",
    "\n",
    "These are the steps taken to remove the background per image:\n",
    "\n",
    "1. We first perform thresholding according to the RGB values. We want the green pixels and the area around it so we filter by taking out the pixels that have too high red and blue values. \n",
    "\n",
    "        1a. We start with 220 threshold (out of 255) and continuously reduce this threshold if the resulting mask image is too big (i.e. not much of the image has been filtered). \n",
    "\n",
    "2. From the resulting mask, we perform morphological techniques like opening so that the pixels in-between the green leaf pixels will be included into the mask (i.e. the brown spots which will most likely be filtered out will be included using the morphological technique). For this, we use a circular structuring element of radius 13px.\n",
    "\n",
    "3. We perform erosion to increase the area of the mask in order for the filter to include the immediate surrounding pixels. Then a final closing operation was done to remove the isolated dots.\n",
    "\n",
    "4. From the blobs formed, we select the largest one and use that as the mask for our actual image.\n",
    "\n",
    "*erosion and opening was used and not dilation and closing since the foreground and background are reversed in the mask (the actual area we want to use has values of False and the backgroudn we want to remove has values of True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:49:39.885299Z",
     "start_time": "2019-12-19T08:49:39.882489Z"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.morphology import binary_closing, binary_opening, erosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T14:14:45.817364Z",
     "start_time": "2019-12-11T14:14:45.811380Z"
    }
   },
   "outputs": [],
   "source": [
    "# reshape_type =  list(map(lambda x: x.replace('/Labelled\\\\', '/Resized\\\\'), labelled_type))\n",
    "# files_reshape = list(map(lambda x: x.replace('/Labelled\\\\', '/Resized\\\\'), files))\n",
    "\n",
    "bgremoved_type =  list(map(lambda x: x.replace('/Labelled\\\\', '/BGRemoved\\\\'), labelled_type))\n",
    "files_bgremoved = list(map(lambda x: x.replace('/Labelled\\\\', '/BGRemoved\\\\'), files))\n",
    "bgremoved1_type = list(map(lambda x: x.replace('/Labelled\\\\', '/BGRemoved1\\\\'), labelled_type))\n",
    "files_bgremoved1 = list(map(lambda x: x.replace('/Labelled\\\\', '/BGRemoved1\\\\'), files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bgremoved path\n",
    "for t in bgremoved_type:\n",
    "    try:\n",
    "        os.makedirs(t)\n",
    "    except FileExistsError:\n",
    "        print(\"File \", t, 'exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T14:14:46.587208Z",
     "start_time": "2019-12-11T14:14:46.580220Z"
    }
   },
   "outputs": [],
   "source": [
    "selem = np.zeros((25, 25))\n",
    "\n",
    "ci,cj=12, 12\n",
    "cr=13\n",
    "\n",
    "# Create index arrays to z\n",
    "I,J=np.meshgrid(np.arange(selem.shape[0]),np.arange(selem.shape[1]))\n",
    "\n",
    "# calculate distance of all points to centre\n",
    "dist=np.sqrt((I-ci)**2+(J-cj)**2)\n",
    "\n",
    "# Assign value of 1 to those points where dist<cr:\n",
    "selem[np.where(dist<=cr)]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:44:46.590736Z",
     "start_time": "2019-12-11T14:51:13.472621Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "# fig, ax = plt.subplots(20,2, figsize=(10,80))\n",
    "idx = 0\n",
    "for file, file_save in zip(files_reshape, files_bgremoved):\n",
    "    bg_frac = 0\n",
    "    thres = 200\n",
    "    img = Image.open(file)\n",
    "    im_arr = np.array(img)\n",
    "#     ax[idx, 0].imshow(im_arr)\n",
    "    R = im_arr[:, :, 0]\n",
    "    G = im_arr[:, :, 1]\n",
    "    B = im_arr[:, :, 2]\n",
    "    while bg_frac < 0.6: \n",
    "        bg_mask = ((R>thres) | (B>thres)) #& (G < 100)\n",
    "        bg_frac = bg_mask.sum()/len(bg_mask.flatten())\n",
    "        thres -= 5\n",
    "    # we use opening first since our mask is reversed (the foreground and background are reversed here)\n",
    "    bg_mask = binary_closing(erosion(binary_opening(bg_mask, selem), np.ones((3, 3))), np.ones((5,5)))\n",
    "    \n",
    "    #Get biggest blob\n",
    "    label, num_label = ndimage.label(~bg_mask)\n",
    "    size = np.bincount(label.ravel())\n",
    "    biggest_label = size[1:].argmax() + 1\n",
    "    bg_mask = label == biggest_label\n",
    "    \n",
    "    im_arr[~bg_mask, 0] = 255\n",
    "    im_arr[~bg_mask, 1] = 255\n",
    "    im_arr[~bg_mask, 2] = 255\n",
    "    \n",
    "    img = Image.fromarray(im_arr)\n",
    "    \n",
    "    img.save(file_save)\n",
    "    idx+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T13:15:45.622608Z",
     "start_time": "2019-12-11T13:15:45.368254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.8980e+03, 3.2000e+02, 3.3400e+02, 4.4900e+02, 4.0600e+02,\n",
       "        1.8310e+03, 7.2720e+03, 1.4600e+02, 1.0000e+00, 3.2519e+04]),\n",
       " array([  0. ,  25.5,  51. ,  76.5, 102. , 127.5, 153. , 178.5, 204. ,\n",
       "        229.5, 255. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(R.flatten(), alpha=0.5)\n",
    "plt.hist(G.flatten(), alpha=0.5)\n",
    "plt.hist(B.flatten(), alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "To account for different environments upon taking the picture, e.g. rotated images, shifted objects, zoomed in pictures, image augmentation is performed. The augmented images will be transformed versions of the dataset by randomly rotating the image up to 30$^\\circ$, shifting the image vertically or horizontally up to 15% of the image length, shearing the image up to 20$^\\circ$, and zooming in the image 0.8x to 1.2x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:16:44.180136Z",
     "start_time": "2019-12-19T08:16:42.185773Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "base_dir = ('D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset/BGRemoved')\n",
    "os.makedirs(base_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:16:47.480955Z",
     "start_time": "2019-12-19T08:16:44.193849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 420 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30, width_shift_range=0.15,\n",
    "    height_shift_range=0.15, shear_range=0.15, \n",
    "    zoom_range=0.2,horizontal_flip=True, \n",
    "    fill_mode=\"nearest\", validation_split=0.3)\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "For the modeling, we used different pre-trained models and one manually made model to classify the images. Six architectures were used for this project:\n",
    "\n",
    "1. VGG16 base for feature extraction and 3-dense layers for classification.\n",
    "2. VGG16 base with only the first 3 blocks frozen and 3-dense layers\n",
    "3. VGG19 base and 3-dense layers\n",
    "4. XCeption base and 3-dense layers\n",
    "5. ResNet base and 3-dense layers\n",
    "6. 5 convolutional layers and 2-dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:28:44.452031Z",
     "start_time": "2019-12-23T11:28:29.784205Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint('VGG16.h5', verbose=1, monitor='val_accuracy', save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:16:47.498908Z",
     "start_time": "2019-12-19T08:16:47.491926Z"
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30, width_shift_range=0.15,\n",
    "    height_shift_range=0.15, shear_range=0.15, \n",
    "    zoom_range=0.2,horizontal_flip=True, \n",
    "    fill_mode=\"nearest\")\n",
    "# datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "def extract_features(trainorval, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 7, 7, 512))\n",
    "    labels = np.zeros(shape=(sample_count, 4))\n",
    "    if trainorval==\"training\":\n",
    "        generator = train_generator\n",
    "    else:\n",
    "        generator = val_generator\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(preprocess_input(inputs_batch))\n",
    "        try:\n",
    "            features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        except ValueError:\n",
    "            break\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:16:48.577485Z",
     "start_time": "2019-12-19T08:16:47.787391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Justin\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:16:49.828759Z",
     "start_time": "2019-12-19T08:16:49.824753Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers:\n",
    "    layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:25:30.078651Z",
     "start_time": "2019-12-19T08:16:51.843433Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features('training', 2319)\n",
    "validation_features, validation_labels = extract_features('validation', 991)\n",
    "\n",
    "train_features = np.reshape(train_features, (2319, 7 * 7 * 512))\n",
    "validation_features = np.reshape(validation_features, (991, 7 * 7 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T08:58:32.982783Z",
     "start_time": "2019-12-23T08:58:32.228356Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=7 * 7 * 512))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))  #Removing 50% of the weights!\n",
    "model.add(layers.Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T08:59:15.375270Z",
     "start_time": "2019-12-23T08:59:15.139233Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "INIT_LR = 1e-1\n",
    "EPOCHS = 30\n",
    "\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T09:30:20.525123Z",
     "start_time": "2019-12-23T09:30:20.377121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:\\\\Users\\\\1000246125\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\Library\\\\bin'\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=1, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:26:49.729940Z",
     "start_time": "2019-12-19T08:25:30.259953Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2319 samples, validate on 991 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Justin\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.3477 - acc: 0.5844\n",
      "Epoch 00001: val_acc improved from -inf to 0.58426, saving model to VGG16.h5\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3451 - acc: 0.5846 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 2/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.3831 - acc: 0.5849\n",
      "Epoch 00002: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 3/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.3933 - acc: 0.5843\n",
      "Epoch 00003: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 4/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.3967 - acc: 0.5841\n",
      "Epoch 00004: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 5/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.3933 - acc: 0.5843\n",
      "Epoch 00005: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 6/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 6.3979 - acc: 0.5840\n",
      "Epoch 00006: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 7/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.4001 - acc: 0.5838\n",
      "Epoch 00007: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 8/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 6.4013 - acc: 0.5838\n",
      "Epoch 00008: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 9/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.4102 - acc: 0.5832\n",
      "Epoch 00009: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 10/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 6.3913 - acc: 0.5844\n",
      "Epoch 00010: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 11/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.4034 - acc: 0.5836\n",
      "Epoch 00011: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 12/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.4034 - acc: 0.5836\n",
      "Epoch 00012: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 13/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 6.3979 - acc: 0.5840\n",
      "Epoch 00013: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 14/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 6.3946 - acc: 0.5842\n",
      "Epoch 00014: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 15/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 6.4013 - acc: 0.5838\n",
      "Epoch 00015: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 16/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.3899 - acc: 0.5845\n",
      "Epoch 00016: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 17/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.3967 - acc: 0.5841\n",
      "Epoch 00017: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 18/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.4068 - acc: 0.5834- ETA: 1s - loss: 6.58\n",
      "Epoch 00018: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 2s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 19/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.4001 - acc: 0.5838\n",
      "Epoch 00019: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 20/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.4136 - acc: 0.5830\n",
      "Epoch 00020: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 21/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.4034 - acc: 0.5836\n",
      "Epoch 00021: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 22/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.4001 - acc: 0.5838\n",
      "Epoch 00022: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 23/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.3899 - acc: 0.5845\n",
      "Epoch 00023: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 24/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.4034 - acc: 0.5836\n",
      "Epoch 00024: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 25/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.3933 - acc: 0.5843\n",
      "Epoch 00025: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 26/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.3933 - acc: 0.5843\n",
      "Epoch 00026: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 27/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.3933 - acc: 0.5843\n",
      "Epoch 00027: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 28/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.3899 - acc: 0.5845\n",
      "Epoch 00028: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 29/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.4001 - acc: 0.5838\n",
      "Epoch 00029: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n",
      "Epoch 30/30\n",
      "2272/2319 [============================>.] - ETA: 0s - loss: 6.4034 - acc: 0.5836\n",
      "Epoch 00030: val_acc did not improve from 0.58426\n",
      "2319/2319 [==============================] - 3s 1ms/sample - loss: 6.3964 - acc: 0.5841 - val_loss: 6.3937 - val_acc: 0.5843\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "INIT_LR = 1e-1\n",
    "EPOCHS = 30\n",
    "\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('VGG16.h5', verbose=1, monitor='val_acc', save_best_only=True, mode='auto') \n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=30,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(validation_features, validation_labels),\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 (Frozen first 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:26:19.707566Z",
     "start_time": "2019-12-18T19:26:18.874210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2319 images belonging to 4 classes.\n",
      "Found 991 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30, width_shift_range=0.15,\n",
    "    height_shift_range=0.15, shear_range=0.15, \n",
    "    zoom_range=0.2,horizontal_flip=True, \n",
    "    fill_mode=\"nearest\", validation_split=0.3)\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:26:23.784869Z",
     "start_time": "2019-12-18T19:26:23.748783Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block4_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T06:59:35.594530Z",
     "start_time": "2019-12-19T06:59:29.974669Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=7 * 7 * 512))\n",
    "# model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "# model.add(layers.Dropout(0.2))  #Removing 50% of the weights!\n",
    "model.add(layers.Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T23:55:16.065526Z",
     "start_time": "2019-12-18T19:26:24.722747Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "71/72 [============================>.] - ETA: 6s - loss: 4.5213 - acc: 0.7051 \n",
      "Epoch 00001: val_acc improved from -inf to 0.72200, saving model to VGG16_frozen4.h5\n",
      "72/72 [==============================] - 550s 8s/step - loss: 4.5388 - acc: 0.7040 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 2/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2951 - acc: 0.7206 \n",
      "Epoch 00002: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 535s 7s/step - loss: 4.2922 - acc: 0.7208 - val_loss: 4.2761 - val_acc: 0.7220\n",
      "Epoch 3/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2723 - acc: 0.7226 \n",
      "Epoch 00003: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 535s 7s/step - loss: 4.2596 - acc: 0.7234 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 4/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.3019 - acc: 0.7202 \n",
      "Epoch 00004: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 537s 7s/step - loss: 4.2922 - acc: 0.7208 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 5/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2613 - acc: 0.7228 \n",
      "Epoch 00005: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 537s 7s/step - loss: 4.2689 - acc: 0.7223 - val_loss: 4.2751 - val_acc: 0.7220\n",
      "Epoch 6/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.3082 - acc: 0.7195 \n",
      "Epoch 00006: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 537s 7s/step - loss: 4.3085 - acc: 0.7195 - val_loss: 4.2749 - val_acc: 0.7220\n",
      "Epoch 7/30\n",
      "71/72 [============================>.] - ETA: 6s - loss: 4.2667 - acc: 0.7217 \n",
      "Epoch 00007: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 551s 8s/step - loss: 4.2477 - acc: 0.7230 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 8/30\n",
      "71/72 [============================>.] - ETA: 6s - loss: 4.2644 - acc: 0.7227 \n",
      "Epoch 00008: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 540s 7s/step - loss: 4.2720 - acc: 0.7222 - val_loss: 4.2749 - val_acc: 0.7220\n",
      "Epoch 9/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.3051 - acc: 0.7196 \n",
      "Epoch 00009: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 533s 7s/step - loss: 4.3054 - acc: 0.7196 - val_loss: 4.2741 - val_acc: 0.7220\n",
      "Epoch 10/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2177 - acc: 0.7259 \n",
      "Epoch 00010: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 535s 7s/step - loss: 4.2325 - acc: 0.7250 - val_loss: 4.2746 - val_acc: 0.7220\n",
      "Epoch 11/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2791 - acc: 0.7222 \n",
      "Epoch 00011: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 536s 7s/step - loss: 4.2797 - acc: 0.7221 - val_loss: 4.2749 - val_acc: 0.7220\n",
      "Epoch 12/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.3084 - acc: 0.7199 \n",
      "Epoch 00012: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 538s 7s/step - loss: 4.3120 - acc: 0.7196 - val_loss: 4.2756 - val_acc: 0.7220\n",
      "Epoch 13/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2509 - acc: 0.7232 \n",
      "Epoch 00013: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 537s 7s/step - loss: 4.2487 - acc: 0.7233 - val_loss: 4.2741 - val_acc: 0.7220\n",
      "Epoch 14/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2782 - acc: 0.7217 \n",
      "Epoch 00014: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 542s 8s/step - loss: 4.2789 - acc: 0.7217 - val_loss: 4.2761 - val_acc: 0.7220\n",
      "Epoch 15/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.3134 - acc: 0.7202 \n",
      "Epoch 00015: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 535s 7s/step - loss: 4.3001 - acc: 0.7210 - val_loss: 4.2756 - val_acc: 0.7220\n",
      "Epoch 16/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2850 - acc: 0.7213 \n",
      "Epoch 00016: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 535s 7s/step - loss: 4.2755 - acc: 0.7219 - val_loss: 4.2759 - val_acc: 0.7220\n",
      "Epoch 17/30\n",
      "71/72 [============================>.] - ETA: 6s - loss: 4.2543 - acc: 0.7234 \n",
      "Epoch 00017: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 539s 7s/step - loss: 4.2620 - acc: 0.7229 - val_loss: 4.2744 - val_acc: 0.7220\n",
      "Epoch 18/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.3141 - acc: 0.7203 \n",
      "Epoch 00018: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 532s 7s/step - loss: 4.3107 - acc: 0.7205 - val_loss: 4.2739 - val_acc: 0.7220\n",
      "Epoch 19/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2566 - acc: 0.7224 \n",
      "Epoch 00019: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 535s 7s/step - loss: 4.2510 - acc: 0.7228 - val_loss: 4.2749 - val_acc: 0.7220\n",
      "Epoch 20/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2227 - acc: 0.7246 \n",
      "Epoch 00020: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 538s 7s/step - loss: 4.2210 - acc: 0.7247 - val_loss: 4.2751 - val_acc: 0.7220\n",
      "Epoch 21/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.3121 - acc: 0.7195 \n",
      "Epoch 00021: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 536s 7s/step - loss: 4.3056 - acc: 0.7199 - val_loss: 4.2761 - val_acc: 0.7220\n",
      "Epoch 22/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.3116 - acc: 0.7193 \n",
      "Epoch 00022: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 537s 7s/step - loss: 4.3085 - acc: 0.7195 - val_loss: 4.2741 - val_acc: 0.7220\n",
      "Epoch 23/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.3048 - acc: 0.7197 \n",
      "Epoch 00023: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 535s 7s/step - loss: 4.2985 - acc: 0.7202 - val_loss: 4.2764 - val_acc: 0.7220\n",
      "Epoch 24/30\n",
      "71/72 [============================>.] - ETA: 6s - loss: 4.1798 - acc: 0.7282 \n",
      "Epoch 00024: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 540s 7s/step - loss: 4.1752 - acc: 0.7285 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 25/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.3330 - acc: 0.7183 \n",
      "Epoch 00025: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 532s 7s/step - loss: 4.3263 - acc: 0.7187 - val_loss: 4.2751 - val_acc: 0.7220\n",
      "Epoch 26/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.3186 - acc: 0.7192 \n",
      "Epoch 00026: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 537s 7s/step - loss: 4.3187 - acc: 0.7192 - val_loss: 4.2759 - val_acc: 0.7220\n",
      "Epoch 27/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2836 - acc: 0.7223 \n",
      "Epoch 00027: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 537s 7s/step - loss: 4.2974 - acc: 0.7214 - val_loss: 4.2751 - val_acc: 0.7220\n",
      "Epoch 28/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2105 - acc: 0.7262 \n",
      "Epoch 00028: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 536s 7s/step - loss: 4.2055 - acc: 0.7265 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 29/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.2613 - acc: 0.7228 \n",
      "Epoch 00029: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 536s 7s/step - loss: 4.2655 - acc: 0.7226 - val_loss: 4.2756 - val_acc: 0.7220\n",
      "Epoch 30/30\n",
      "71/72 [============================>.] - ETA: 5s - loss: 4.3024 - acc: 0.7204 \n",
      "Epoch 00030: val_acc did not improve from 0.72200\n",
      "72/72 [==============================] - 535s 7s/step - loss: 4.3060 - acc: 0.7202 - val_loss: 4.2759 - val_acc: 0.7220\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "INIT_LR = 1e-3\n",
    "EPOCHS = 30\n",
    "\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('VGG16_frozen3.h5', verbose=1, monitor='val_acc', save_best_only=True, mode='auto') \n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                    epochs=30,\n",
    "                    steps_per_epoch=2319 // batch_size,\n",
    "                    validation_data=val_generator,\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T01:37:59.154795Z",
     "start_time": "2019-12-18T01:37:58.792930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2351 images belonging to 4 classes.\n",
      "Found 1004 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30, width_shift_range=0.15,\n",
    "    height_shift_range=0.15, shear_range=0.15, \n",
    "    zoom_range=0.2,horizontal_flip=True, \n",
    "    fill_mode=\"nearest\", validation_split=0.3)\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:20:53.549141Z",
     "start_time": "2019-12-17T20:20:53.542160Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features19(trainorval, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 7, 7, 512))\n",
    "    labels = np.zeros(shape=(sample_count, 4))\n",
    "    if trainorval==\"training\":\n",
    "        generator = train_generator\n",
    "    else:\n",
    "        generator = val_generator\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base19.predict(preprocess_input(inputs_batch))\n",
    "        try:\n",
    "            features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        except ValueError:\n",
    "            break\n",
    "        if i==0:\n",
    "            print(\"one down\")\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:20:54.743492Z",
     "start_time": "2019-12-17T20:20:53.975497Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
    "\n",
    "conv_base19 = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T01:21:12.543085Z",
     "start_time": "2019-12-18T01:21:12.538131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one down\n",
      "one down\n"
     ]
    }
   ],
   "source": [
    "train_features19, train_labels19 = extract_features19('training', 2351)\n",
    "validation_features19, validation_labels19 = extract_features19('validation', 1004)\n",
    "\n",
    "train_features19 = np.reshape(train_features19, (2351, 7 * 7 * 512))\n",
    "validation_features19 = np.reshape(validation_features19, (1004, 7 * 7 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T02:38:52.958494Z",
     "start_time": "2019-12-18T02:38:52.815287Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=7 * 7 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))  #Removing 50% of the weights!\n",
    "model.add(layers.Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T02:40:20.354185Z",
     "start_time": "2019-12-18T02:38:53.081709Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2351 samples, validate on 1004 samples\n",
      "Epoch 1/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.3105 - accuracy: 0.7032WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 2s 852us/sample - loss: 4.2961 - accuracy: 0.7043 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 2/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2796 - accuracy: 0.7198WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 572us/sample - loss: 4.2686 - accuracy: 0.7205 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 3/30\n",
      "2304/2351 [============================>.] - ETA: 0s - loss: 4.2655 - accuracy: 0.7216WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 578us/sample - loss: 4.2650 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 4/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2626 - accuracy: 0.7220WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 579us/sample - loss: 4.2713 - accuracy: 0.7214 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 5/30\n",
      "2240/2351 [===========================>..] - ETA: 0s - loss: 4.2930 - accuracy: 0.7201 ETA: 0s - loss: 4.2793 WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 2s 653us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 6/30\n",
      "2304/2351 [============================>.] - ETA: 0s - loss: 4.2967 - accuracy: 0.7198WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 613us/sample - loss: 4.2695 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 7/30\n",
      "2240/2351 [===========================>..] - ETA: 0s - loss: 4.2761 - accuracy: 0.7210 ETA: 1s - lWARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 617us/sample - loss: 4.2797 - accuracy: 0.7208 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 8/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2732 - accuracy: 0.7213 ETA: 0s - loss: 4.3820 - accuracy: 0.71 - ETA: 0s - loss: 4.3451 - accuWARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 626us/sample - loss: 4.2720 - accuracy: 0.7214 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 9/30\n",
      "2240/2351 [===========================>..] - ETA: 0s - loss: 4.2485 - accuracy: 0.7230WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 619us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 10/30\n",
      "2240/2351 [===========================>..] - ETA: 0s - loss: 4.2654 - accuracy: 0.7219WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 622us/sample - loss: 4.2696 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 11/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2807 - accuracy: 0.7209 ETA: 0s - loss: 4.2083 - accuracy:  - ETA: 0s - loss: 4.2457 - accuraWARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 610us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 12/30\n",
      "2240/2351 [===========================>..] - ETA: 0s - loss: 4.2999 - accuracy: 0.7196WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 625us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 13/30\n",
      "2272/2351 [===========================>..] - ETA: 0s - loss: 4.2899 - accuracy: 0.7203 ETA: 0s - loss: 4WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 616us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 14/30\n",
      "2304/2351 [============================>.] - ETA: 0s - loss: 4.2959 - accuracy: 0.7199 ETA: 0s - loss: 4.2 - ETA: 0s - loss: 4.2869 - accuracy: 0.7205WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 617us/sample - loss: 4.2730 - accuracy: 0.7214 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 15/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2676 - accuracy: 0.7217WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 635us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 16/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2643 - accuracy: 0.7220WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 633us/sample - loss: 4.2665 - accuracy: 0.7218 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 17/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2772 - accuracy: 0.7209WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 622us/sample - loss: 4.2727 - accuracy: 0.7212 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 18/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2865 - accuracy: 0.7200 ETA: 0s - losWARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 610us/sample - loss: 4.2819 - accuracy: 0.7203 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 19/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2997 - accuracy: 0.71 - ETA: 0s - loss: 4.2742 - accuracy: 0.7213WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 612us/sample - loss: 4.2730 - accuracy: 0.7214 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 20/30\n",
      "2240/2351 [===========================>..] - ETA: 0s - loss: 4.2885 - accuracy: 0.7203 ETA: 0s - loss: 4.1WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 617us/sample - loss: 4.2719 - accuracy: 0.7214 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 21/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2758 - accuracy: 0.7211WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 608us/sample - loss: 4.2713 - accuracy: 0.7214 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 22/30\n",
      "2272/2351 [===========================>..] - ETA: 0s - loss: 4.2946 - accuracy: 0.7199WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 613us/sample - loss: 4.2709 - accuracy: 0.7214 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 23/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2873 - accuracy: 0.7205WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 609us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 24/30\n",
      "2304/2351 [============================>.] - ETA: 0s - loss: 4.2783 - accuracy: 0.72 - ETA: 0s - loss: 4.2736 - accuracy: 0.7214WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 612us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 25/30\n",
      "2304/2351 [============================>.] - ETA: 0s - loss: 4.2636 - accuracy: 0.7220WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 617us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2742 - accuracy: 0.7213WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 607us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 27/30\n",
      "2240/2351 [===========================>..] - ETA: 0s - loss: 4.2519 - accuracy: 0.7228 ETA: 0s - loss: 4.3WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 622us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 28/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2610 - accuracy: 0.7222 ETA: 0s - loss: 4WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 609us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 29/30\n",
      "2336/2351 [============================>.] - ETA: 0s - loss: 4.2643 - accuracy: 0.7220WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 605us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n",
      "Epoch 30/30\n",
      "2304/2351 [============================>.] - ETA: 0s - loss: 4.2636 - accuracy: 0.7220 ETA: 0s - lWARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "2351/2351 [==============================] - 1s 615us/sample - loss: 4.2697 - accuracy: 0.7216 - val_loss: 4.2620 - val_accuracy: 0.7221\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "INIT_LR = 1e-3\n",
    "EPOCHS = 30\n",
    "\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('VGG19.h5', verbose=1, monitor='val_acc', save_best_only=True, mode='auto') \n",
    "\n",
    "history = model.fit(train_features19, train_labels19,\n",
    "                    epochs=30,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(validation_features19, validation_labels19),\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 0s 277us/sample - loss: 7.6685 - accuracy: 0.5000\n",
      "Test Accuracy: 50.0\n"
     ]
    }
   ],
   "source": [
    "scores_brownspot = model.evaluate(train_features19[train_labels19[:,0]==1], train_labels19[train_labels19[:,0]==1])\n",
    "print(f\"Test Accuracy: {scores_brownspot[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1042/1042 [==============================] - 0s 265us/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Test Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "scores_healthy = model.evaluate(train_features19[train_labels19[:,1]==1], train_labels19[train_labels19[:,1]==1])\n",
    "print(f\"Test Accuracy: {scores_healthy[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 343us/sample - loss: 7.6685 - accuracy: 0.5000\n",
      "Test Accuracy: 50.0\n"
     ]
    }
   ],
   "source": [
    "scores_hispa = model.evaluate(train_features19[train_labels19[:,2]==1], train_labels19[train_labels19[:,2]==1])\n",
    "print(f\"Test Accuracy: {scores_hispa[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 0s 297us/sample - loss: 7.6685 - accuracy: 0.5000\n",
      "Test Accuracy: 50.0\n"
     ]
    }
   ],
   "source": [
    "scores_leafblast = model.evaluate(train_features19[train_labels19[:,3]==1], train_labels19[train_labels19[:,3]==1])\n",
    "print(f\"Test Accuracy: {scores_leafblast[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/LabelledRice/vgg19.model\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x281caf72ec8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = model.load('best_model_5conv2dense.h5')\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# json_file = open('D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/LabelledRice/rice_disease_classifer.model', 'r')\n",
    "# loaded_model = model.load\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# loaded_model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"best_model_5conv2dense_woutBG.h5\")\n",
    "model1 = load_model(\"D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/LabelledRice/vgg19.model\")\n",
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XCeption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T01:37:59.154795Z",
     "start_time": "2019-12-18T01:37:58.792930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2319 images belonging to 4 classes.\n",
      "Found 991 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30, width_shift_range=0.15,\n",
    "    height_shift_range=0.15, shear_range=0.15, \n",
    "    zoom_range=0.2,horizontal_flip=True, \n",
    "    fill_mode=\"nearest\", validation_split=0.3)\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T01:23:39.106815Z",
     "start_time": "2019-12-18T01:23:39.096710Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T01:23:43.225755Z",
     "start_time": "2019-12-18T01:23:39.202559Z"
    }
   },
   "outputs": [],
   "source": [
    "xception_base = Xception(weights='imagenet',\n",
    "                         include_top=False, \n",
    "                         input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T01:23:43.332492Z",
     "start_time": "2019-12-18T01:23:43.326476Z"
    }
   },
   "outputs": [],
   "source": [
    "# datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "def extract_features_xception(trainorval, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 7, 7, 2048))\n",
    "    labels = np.zeros(shape=(sample_count, 4))\n",
    "    if trainorval==\"training\":\n",
    "        generator = train_generator\n",
    "    else:\n",
    "        generator = val_generator\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = xception_base.predict(preprocess_input(inputs_batch))\n",
    "        try:\n",
    "            features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        except ValueError:\n",
    "            break\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T01:30:37.121416Z",
     "start_time": "2019-12-18T01:23:43.420225Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features_x, train_labels_x = extract_features_xception('training', 2319)\n",
    "validation_features_x, validation_labels_x = extract_features_xception('validation', 991)\n",
    "\n",
    "train_features_x = np.reshape(train_features_x, (2319, 7 * 7 * 2048))\n",
    "validation_features_x = np.reshape(validation_features_x, (991, 7 * 7 * 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T03:14:03.825153Z",
     "start_time": "2019-12-18T03:14:02.359470Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=7 * 7 * 2048))\n",
    "model.add(layers.Dropout(0.35))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.35))  #Removing 50% of the weights!\n",
    "model.add(layers.Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T03:23:38.810529Z",
     "start_time": "2019-12-18T03:14:03.983720Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2319 samples, validate on 991 samples\n",
      "Epoch 1/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.1240 - acc: 0.7145\n",
      "Epoch 00001: val_acc improved from -inf to 0.72402, saving model to XCeption.h5\n",
      "2319/2319 [==============================] - 141s 61ms/sample - loss: 4.1254 - acc: 0.7144 - val_loss: 4.2110 - val_acc: 0.7240\n",
      "Epoch 2/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.3414 - acc: 0.7130\n",
      "Epoch 00002: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 20s 9ms/sample - loss: 4.3390 - acc: 0.7132 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 3/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2730 - acc: 0.7220\n",
      "Epoch 00003: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2745 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 4/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2801 - acc: 0.7211\n",
      "Epoch 00004: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2716 - acc: 0.7216 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 5/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2827 - acc: 0.7215\n",
      "Epoch 00005: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 6/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2694 - acc: 0.7224\n",
      "Epoch 00006: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 14s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 7/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2660 - acc: 0.7226\n",
      "Epoch 00007: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 14s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 8/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2761 - acc: 0.7220\n",
      "Epoch 00008: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 9/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2776 - acc: 0.7217\n",
      "Epoch 00009: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 14s 6ms/sample - loss: 4.2791 - acc: 0.7216 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 10/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2761 - acc: 0.7220\n",
      "Epoch 00010: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 14s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 11/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2761 - acc: 0.7220\n",
      "Epoch 00011: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 14s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 12/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2794 - acc: 0.7217\n",
      "Epoch 00012: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 14s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 13/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2794 - acc: 0.7217\n",
      "Epoch 00013: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 14/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2794 - acc: 0.7217\n",
      "Epoch 00014: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 15/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2794 - acc: 0.7217\n",
      "Epoch 00015: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 16/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2761 - acc: 0.7220\n",
      "Epoch 00016: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 17/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2761 - acc: 0.7220\n",
      "Epoch 00017: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 18/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2727 - acc: 0.7222\n",
      "Epoch 00018: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 14s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 19/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2761 - acc: 0.7220\n",
      "Epoch 00019: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 14s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 20/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2894 - acc: 0.7211\n",
      "Epoch 00020: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 14s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 21/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2794 - acc: 0.7217\n",
      "Epoch 00021: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 14s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 22/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2794 - acc: 0.7217\n",
      "Epoch 00022: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 23/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2660 - acc: 0.7226\n",
      "Epoch 00023: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 24/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2761 - acc: 0.7220\n",
      "Epoch 00024: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 7ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 25/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2627 - acc: 0.7228\n",
      "Epoch 00025: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 26/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2861 - acc: 0.7213\n",
      "Epoch 00026: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 14s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 27/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2794 - acc: 0.7217\n",
      "Epoch 00027: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 28/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2761 - acc: 0.7220\n",
      "Epoch 00028: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 7ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 29/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2593 - acc: 0.7230\n",
      "Epoch 00029: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 6ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 30/30\n",
      "2300/2319 [============================>.] - ETA: 0s - loss: 4.2827 - acc: 0.7215\n",
      "Epoch 00030: val_acc did not improve from 0.72402\n",
      "2319/2319 [==============================] - 15s 7ms/sample - loss: 4.2775 - acc: 0.7219 - val_loss: 4.2754 - val_acc: 0.7220\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "INIT_LR = 1e-3\n",
    "EPOCHS = 30\n",
    "\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('XCeption.h5', verbose=1, monitor='val_acc', save_best_only=True, mode='auto') \n",
    "\n",
    "history = model.fit(train_features_x, train_labels_x,\n",
    "                    epochs=30,\n",
    "                    batch_size=20,\n",
    "                    validation_data=(validation_features_x, validation_labels_x),\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T01:37:59.154795Z",
     "start_time": "2019-12-18T01:37:58.792930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2319 images belonging to 4 classes.\n",
      "Found 991 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30, width_shift_range=0.15,\n",
    "    height_shift_range=0.15, shear_range=0.15, \n",
    "    zoom_range=0.2,horizontal_flip=True, \n",
    "    fill_mode=\"nearest\", validation_split=0.3)\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T01:37:59.337104Z",
     "start_time": "2019-12-18T01:37:59.253531Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T01:38:06.454577Z",
     "start_time": "2019-12-18T01:37:59.433514Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "resnet_base = ResNet50(weights='imagenet',\n",
    "                         include_top=False,\n",
    "                         input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T02:31:27.300438Z",
     "start_time": "2019-12-18T02:31:27.295436Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features_resnet(trainorval, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 7, 7, 2048))\n",
    "    labels = np.zeros(shape=(sample_count, 4))\n",
    "    if trainorval==\"training\":\n",
    "        generator = train_generator\n",
    "    else:\n",
    "        generator = val_generator\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = resnet_base.predict(preprocess_input(inputs_batch))\n",
    "        try:\n",
    "            features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        except ValueError:\n",
    "            break\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T02:51:48.522294Z",
     "start_time": "2019-12-18T02:49:10.614430Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features_res, train_labels_res = extract_features_resnet('training', 2319)\n",
    "validation_features_res, validation_labels_res = extract_features_resnet('validation', 991)\n",
    "\n",
    "train_features_res = np.reshape(train_features_res, (2319, 7 * 7 * 2048))\n",
    "validation_features_res = np.reshape(validation_features_res, (991, 7 * 7 * 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T03:28:00.058495Z",
     "start_time": "2019-12-18T03:27:59.435102Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=7 * 7 * 2048))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.4))  #Removing 50% of the weights!\n",
    "model.add(layers.Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T03:33:10.338797Z",
     "start_time": "2019-12-18T03:28:00.331306Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2319 samples, validate on 991 samples\n",
      "Epoch 1/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6092 - acc: 0.9774\n",
      "Epoch 00001: val_acc improved from -inf to 0.57820, saving model to ResNet.h5\n",
      "2319/2319 [==============================] - 18s 8ms/sample - loss: 0.6071 - acc: 0.9776 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 2/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6308 - acc: 0.9766\n",
      "Epoch 00002: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6286 - acc: 0.9767 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 3/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6288 - acc: 0.9768\n",
      "Epoch 00003: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 4/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.9772\n",
      "Epoch 00004: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 5/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6256 - acc: 0.9770\n",
      "Epoch 00005: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 6/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6246 - acc: 0.9770\n",
      "Epoch 00006: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6256 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 7/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6256 - acc: 0.9770\n",
      "Epoch 00007: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 8/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6288 - acc: 0.9768\n",
      "Epoch 00008: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 9/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6192 - acc: 0.9774\n",
      "Epoch 00009: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 10/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6288 - acc: 0.9768\n",
      "Epoch 00010: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 11/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6288 - acc: 0.9768\n",
      "Epoch 00011: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 12/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6256 - acc: 0.9770\n",
      "Epoch 00012: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 13/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6256 - acc: 0.9770\n",
      "Epoch 00013: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 14/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.9772\n",
      "Epoch 00014: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 15/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6288 - acc: 0.9768\n",
      "Epoch 00015: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 16/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6288 - acc: 0.9768\n",
      "Epoch 00016: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 17/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6256 - acc: 0.9770\n",
      "Epoch 00017: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 18/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6288 - acc: 0.9768\n",
      "Epoch 00018: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 19/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.9772\n",
      "Epoch 00019: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 20/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6289 - acc: 0.9768\n",
      "Epoch 00020: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 9s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 21/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6256 - acc: 0.9770\n",
      "Epoch 00021: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 22/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6288 - acc: 0.9768\n",
      "Epoch 00022: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 23/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6256 - acc: 0.9770\n",
      "Epoch 00023: val_acc did not improve from 0.57820\n",
      "2319/2319 [==============================] - 9s 4ms/sample - loss: 0.6266 - acc: 0.9769 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 24/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6059 - acc: 0.9781\n",
      "Epoch 00024: val_acc improved from 0.57820 to 0.72200, saving model to ResNet.h5\n",
      "2319/2319 [==============================] - 11s 5ms/sample - loss: 0.6038 - acc: 0.9782 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 25/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.9783\n",
      "Epoch 00025: val_acc did not improve from 0.72200\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6012 - acc: 0.9784 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 26/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.6255 - acc: 0.9770\n",
      "Epoch 00026: val_acc did not improve from 0.72200\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.6233 - acc: 0.9771 - val_loss: 6.4868 - val_acc: 0.5782\n",
      "Epoch 27/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.5820 - acc: 0.9798\n",
      "Epoch 00027: val_acc did not improve from 0.72200\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.5801 - acc: 0.9799 - val_loss: 4.8507 - val_acc: 0.6826\n",
      "Epoch 28/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.9850\n",
      "Epoch 00028: val_acc did not improve from 0.72200\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.4986 - acc: 0.9851 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 29/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.4953 - acc: 0.9855\n",
      "Epoch 00029: val_acc did not improve from 0.72200\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.4940 - acc: 0.9856 - val_loss: 4.2754 - val_acc: 0.7220\n",
      "Epoch 30/30\n",
      "2304/2319 [============================>.] - ETA: 0s - loss: 0.4927 - acc: 0.9855\n",
      "Epoch 00030: val_acc did not improve from 0.72200\n",
      "2319/2319 [==============================] - 10s 4ms/sample - loss: 0.4913 - acc: 0.9856 - val_loss: 4.2754 - val_acc: 0.7220\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "INIT_LR = 1e-3\n",
    "EPOCHS = 30\n",
    "\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('ResNet.h5', verbose=1, monitor='val_acc', save_best_only=True, mode='auto') \n",
    "\n",
    "\n",
    "history = model.fit(train_features_res, train_labels_res,\n",
    "                    epochs=30,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(validation_features_res, validation_labels_res),\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5Conv2Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:11:16.047454Z",
     "start_time": "2019-12-19T08:11:16.043913Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "#import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Flatten, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:15:45.099084Z",
     "start_time": "2019-12-11T19:15:45.096042Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "INIT_LR = 1e-3\n",
    "BS = 16\n",
    "default_image_size = tuple((256, 256))\n",
    "image_size = 0\n",
    "directory_root = 'D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset/BGRemoved'\n",
    "width=256\n",
    "height=256\n",
    "depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:15:45.868254Z",
     "start_time": "2019-12-11T19:15:45.863268Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None :\n",
    "            image = cv2.resize(image, default_image_size)   \n",
    "            return img_to_array(image)\n",
    "        else :\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:18:43.070358Z",
     "start_time": "2019-12-11T19:15:46.501234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading images ...\n",
      "[INFO] Image loading completed\n"
     ]
    }
   ],
   "source": [
    "image_list, label_list = [], []\n",
    "try:\n",
    "    print(\"[INFO] Loading images ...\")\n",
    "    root_dir = listdir(directory_root)\n",
    "    for directory in root_dir :\n",
    "        # remove .DS_Store from list\n",
    "        if directory == \".DS_Store\" :\n",
    "            root_dir.remove(directory)\n",
    "\n",
    "    for disease_folder in root_dir :\n",
    "        plant_disease_image_list = listdir(f\"{directory_root}/{disease_folder}\")\n",
    "        \n",
    "        for image in plant_disease_image_list:\n",
    "            image_directory = f\"{directory_root}/{disease_folder}/{image}\"\n",
    "            if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".JPG\") == True:\n",
    "                image_list.append(convert_image_to_array(image_directory))\n",
    "                label_list.append(disease_folder)\n",
    "    print(\"[INFO] Image loading completed\")  \n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:18:45.552496Z",
     "start_time": "2019-12-11T19:18:45.549238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "image_size = len(image_list)\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:18:47.982309Z",
     "start_time": "2019-12-11T19:18:47.963462Z"
    }
   },
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "image_labels = label_binarizer.fit_transform(label_list)\n",
    "pickle.dump(label_binarizer,open('label_transform.pkl', 'wb'))\n",
    "n_classes = len(label_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:18:50.758548Z",
     "start_time": "2019-12-11T19:18:50.755288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BrownSpot' 'Healthy' 'Hispa' 'LeafBlast']\n"
     ]
    }
   ],
   "source": [
    "print(label_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:19:00.711405Z",
     "start_time": "2019-12-11T19:18:53.230119Z"
    }
   },
   "outputs": [],
   "source": [
    "np_image_list = np.array(image_list, dtype=np.float16) / 225.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:19:05.068392Z",
     "start_time": "2019-12-11T19:19:03.967235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Spliting data to train, test\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Spliting data to train, test\")\n",
    "x_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.3, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:19:09.314173Z",
     "start_time": "2019-12-11T19:19:08.972810Z"
    }
   },
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(\n",
    "    rotation_range=30, width_shift_range=0.15,\n",
    "    height_shift_range=0.15, shear_range=0.15, \n",
    "    zoom_range=0.2,horizontal_flip=True, \n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:19:33.359424Z",
     "start_time": "2019-12-11T19:19:11.840937Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "inputShape = (height, width, depth)\n",
    "chanDim = -1\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    inputShape = (depth, height, width)\n",
    "    chanDim = 1\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T18:09:04.787483Z",
     "start_time": "2019-12-11T18:09:04.778518Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 85, 85, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 85, 85, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 85, 85, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 85, 85, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 85, 85, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 85, 85, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 85, 85, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 85, 85, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 42, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 42, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 42, 42, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 42, 42, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 42, 42, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 42, 42, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 42, 42, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 42, 42, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56448)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               14450944  \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 14,732,420\n",
      "Trainable params: 14,731,076\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:19:36.562519Z",
     "start_time": "2019-12-11T19:19:36.397626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "# distribution\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:19:39.170046Z",
     "start_time": "2019-12-11T19:19:39.109174Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model_5conv2dense_woutBG.h5', verbose=1, monitor='val_accuracy', save_best_only=True, mode='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:40:01.636755Z",
     "start_time": "2019-12-11T19:19:41.643475Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_10616/1435869103.py:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 26 steps, validate on 180 samples\n",
      "Epoch 1/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.8600 - accuracy: 0.6688\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.62500, saving model to best_model_5conv2dense_woutBG.h5\n",
      "26/26 [==============================] - 22s 860ms/step - loss: 0.8573 - accuracy: 0.6696 - val_loss: 1.3132 - val_accuracy: 0.6250\n",
      "Epoch 2/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6756 - accuracy: 0.7274\n",
      "Epoch 00002: val_accuracy improved from 0.62500 to 0.65000, saving model to best_model_5conv2dense_woutBG.h5\n",
      "26/26 [==============================] - 17s 658ms/step - loss: 0.6793 - accuracy: 0.7259 - val_loss: 1.5628 - val_accuracy: 0.6500\n",
      "Epoch 3/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6842 - accuracy: 0.7017\n",
      "Epoch 00003: val_accuracy did not improve from 0.65000\n",
      "26/26 [==============================] - 11s 410ms/step - loss: 0.6880 - accuracy: 0.6999 - val_loss: 1.8589 - val_accuracy: 0.6458\n",
      "Epoch 4/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.7039 - accuracy: 0.6812\n",
      "Epoch 00004: val_accuracy did not improve from 0.65000\n",
      "26/26 [==============================] - 11s 441ms/step - loss: 0.7019 - accuracy: 0.6821 - val_loss: 3.6199 - val_accuracy: 0.6250\n",
      "Epoch 5/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6340 - accuracy: 0.7249\n",
      "Epoch 00005: val_accuracy did not improve from 0.65000\n",
      "26/26 [==============================] - 10s 395ms/step - loss: 0.6299 - accuracy: 0.7259 - val_loss: 5.6439 - val_accuracy: 0.6250\n",
      "Epoch 6/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6268 - accuracy: 0.7326\n",
      "Epoch 00006: val_accuracy did not improve from 0.65000\n",
      "26/26 [==============================] - 11s 428ms/step - loss: 0.6228 - accuracy: 0.7339 - val_loss: 4.4855 - val_accuracy: 0.6361\n",
      "Epoch 7/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5584 - accuracy: 0.7487\n",
      "Epoch 00007: val_accuracy did not improve from 0.65000\n",
      "26/26 [==============================] - 11s 441ms/step - loss: 0.5605 - accuracy: 0.7488 - val_loss: 2.3309 - val_accuracy: 0.6278\n",
      "Epoch 8/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5131 - accuracy: 0.7751\n",
      "Epoch 00008: val_accuracy improved from 0.65000 to 0.67083, saving model to best_model_5conv2dense_woutBG.h5\n",
      "26/26 [==============================] - 17s 667ms/step - loss: 0.5207 - accuracy: 0.7710 - val_loss: 3.5277 - val_accuracy: 0.6708\n",
      "Epoch 9/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5610 - accuracy: 0.7371\n",
      "Epoch 00009: val_accuracy did not improve from 0.67083\n",
      "26/26 [==============================] - 12s 477ms/step - loss: 0.5581 - accuracy: 0.7389 - val_loss: 3.5219 - val_accuracy: 0.6556\n",
      "Epoch 10/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5185 - accuracy: 0.7558\n",
      "Epoch 00010: val_accuracy did not improve from 0.67083\n",
      "26/26 [==============================] - 14s 546ms/step - loss: 0.5150 - accuracy: 0.7568 - val_loss: 5.8792 - val_accuracy: 0.6167\n",
      "Epoch 11/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5443 - accuracy: 0.7597\n",
      "Epoch 00011: val_accuracy did not improve from 0.67083\n",
      "26/26 [==============================] - 14s 524ms/step - loss: 0.5446 - accuracy: 0.7605 - val_loss: 5.8792 - val_accuracy: 0.6167\n",
      "Epoch 12/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5035 - accuracy: 0.7635\n",
      "Epoch 00012: val_accuracy did not improve from 0.67083\n",
      "26/26 [==============================] - 16s 598ms/step - loss: 0.5031 - accuracy: 0.7636 - val_loss: 5.1601 - val_accuracy: 0.6222\n",
      "Epoch 13/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5182 - accuracy: 0.7655\n",
      "Epoch 00013: val_accuracy did not improve from 0.67083\n",
      "26/26 [==============================] - 12s 477ms/step - loss: 0.5235 - accuracy: 0.7630 - val_loss: 1.9308 - val_accuracy: 0.6514\n",
      "Epoch 14/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6670 - accuracy: 0.7144\n",
      "Epoch 00014: val_accuracy did not improve from 0.67083\n",
      "26/26 [==============================] - 18s 700ms/step - loss: 0.6612 - accuracy: 0.7151 - val_loss: 5.3065 - val_accuracy: 0.6472\n",
      "Epoch 15/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6218 - accuracy: 0.7390\n",
      "Epoch 00015: val_accuracy did not improve from 0.67083\n",
      "26/26 [==============================] - 13s 516ms/step - loss: 0.6134 - accuracy: 0.7407 - val_loss: 2.3979 - val_accuracy: 0.6472\n",
      "Epoch 16/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5757 - accuracy: 0.7436\n",
      "Epoch 00016: val_accuracy did not improve from 0.67083\n",
      "26/26 [==============================] - 13s 504ms/step - loss: 0.5737 - accuracy: 0.7444 - val_loss: 3.9090 - val_accuracy: 0.6167\n",
      "Epoch 17/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5727 - accuracy: 0.7442\n",
      "Epoch 00017: val_accuracy did not improve from 0.67083\n",
      "26/26 [==============================] - 12s 473ms/step - loss: 0.5751 - accuracy: 0.7432 - val_loss: 3.4453 - val_accuracy: 0.6167\n",
      "Epoch 18/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5644 - accuracy: 0.7410\n",
      "Epoch 00018: val_accuracy improved from 0.67083 to 0.70556, saving model to best_model_5conv2dense_woutBG.h5\n",
      "26/26 [==============================] - 22s 842ms/step - loss: 0.5641 - accuracy: 0.7401 - val_loss: 0.6054 - val_accuracy: 0.7056\n",
      "Epoch 19/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5302 - accuracy: 0.7539\n",
      "Epoch 00019: val_accuracy did not improve from 0.70556\n",
      "26/26 [==============================] - 15s 563ms/step - loss: 0.5285 - accuracy: 0.7556 - val_loss: 1.0011 - val_accuracy: 0.6431\n",
      "Epoch 20/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5022 - accuracy: 0.7674\n",
      "Epoch 00020: val_accuracy did not improve from 0.70556\n",
      "26/26 [==============================] - 13s 497ms/step - loss: 0.5097 - accuracy: 0.7624 - val_loss: 0.8587 - val_accuracy: 0.6653\n",
      "Epoch 21/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5054 - accuracy: 0.7642\n",
      "Epoch 00021: val_accuracy did not improve from 0.70556\n",
      "26/26 [==============================] - 14s 541ms/step - loss: 0.5038 - accuracy: 0.7649 - val_loss: 1.8230 - val_accuracy: 0.6333\n",
      "Epoch 22/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5295 - accuracy: 0.7661\n",
      "Epoch 00022: val_accuracy did not improve from 0.70556\n",
      "26/26 [==============================] - 14s 541ms/step - loss: 0.5270 - accuracy: 0.7679 - val_loss: 1.7915 - val_accuracy: 0.6694\n",
      "Epoch 23/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4997 - accuracy: 0.7642\n",
      "Epoch 00023: val_accuracy improved from 0.70556 to 0.73056, saving model to best_model_5conv2dense_woutBG.h5\n",
      "26/26 [==============================] - 23s 899ms/step - loss: 0.5002 - accuracy: 0.7673 - val_loss: 0.6420 - val_accuracy: 0.7306\n",
      "Epoch 24/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4874 - accuracy: 0.7706\n",
      "Epoch 00024: val_accuracy did not improve from 0.73056\n",
      "26/26 [==============================] - 17s 670ms/step - loss: 0.4881 - accuracy: 0.7704 - val_loss: 0.8169 - val_accuracy: 0.6833\n",
      "Epoch 25/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4806 - accuracy: 0.7713\n",
      "Epoch 00025: val_accuracy improved from 0.73056 to 0.76667, saving model to best_model_5conv2dense_woutBG.h5\n",
      "26/26 [==============================] - 26s 1s/step - loss: 0.4788 - accuracy: 0.7729 - val_loss: 0.5641 - val_accuracy: 0.7667\n",
      "Epoch 26/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5063 - accuracy: 0.7571\n",
      "Epoch 00026: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 15s 572ms/step - loss: 0.4985 - accuracy: 0.7611 - val_loss: 2.0039 - val_accuracy: 0.6639\n",
      "Epoch 27/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5126 - accuracy: 0.7719\n",
      "Epoch 00027: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 18s 684ms/step - loss: 0.5071 - accuracy: 0.7754 - val_loss: 0.8608 - val_accuracy: 0.6736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4850 - accuracy: 0.7706\n",
      "Epoch 00028: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 18s 709ms/step - loss: 0.4832 - accuracy: 0.7704 - val_loss: 1.3434 - val_accuracy: 0.6500\n",
      "Epoch 29/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5030 - accuracy: 0.7629\n",
      "Epoch 00029: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 16s 617ms/step - loss: 0.4987 - accuracy: 0.7655 - val_loss: 2.4031 - val_accuracy: 0.6208\n",
      "Epoch 30/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4885 - accuracy: 0.7732\n",
      "Epoch 00030: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 17s 642ms/step - loss: 0.4854 - accuracy: 0.7754 - val_loss: 0.9157 - val_accuracy: 0.6597\n",
      "Epoch 31/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4486 - accuracy: 0.7816\n",
      "Epoch 00031: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 18s 710ms/step - loss: 0.4555 - accuracy: 0.7797 - val_loss: 2.1415 - val_accuracy: 0.6500\n",
      "Epoch 32/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4693 - accuracy: 0.7835\n",
      "Epoch 00032: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 19s 740ms/step - loss: 0.4670 - accuracy: 0.7816 - val_loss: 0.8079 - val_accuracy: 0.6764\n",
      "Epoch 33/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4465 - accuracy: 0.7977\n",
      "Epoch 00033: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 22s 860ms/step - loss: 0.4491 - accuracy: 0.7958 - val_loss: 1.3625 - val_accuracy: 0.6542\n",
      "Epoch 34/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4614 - accuracy: 0.7812\n",
      "Epoch 00034: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 22s 838ms/step - loss: 0.4634 - accuracy: 0.7812 - val_loss: 2.7201 - val_accuracy: 0.6528\n",
      "Epoch 35/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4441 - accuracy: 0.7867\n",
      "Epoch 00035: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 24s 909ms/step - loss: 0.4435 - accuracy: 0.7871 - val_loss: 2.3200 - val_accuracy: 0.6500\n",
      "Epoch 36/50\n",
      "25/26 [===========================>..] - ETA: 1s - loss: 0.4669 - accuracy: 0.7777\n",
      "Epoch 00036: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4705 - accuracy: 0.7741 - val_loss: 2.6164 - val_accuracy: 0.6194\n",
      "Epoch 37/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4443 - accuracy: 0.7874\n",
      "Epoch 00037: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 22s 864ms/step - loss: 0.4464 - accuracy: 0.7871 - val_loss: 2.3830 - val_accuracy: 0.6236\n",
      "Epoch 38/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4885 - accuracy: 0.7629\n",
      "Epoch 00038: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 23s 870ms/step - loss: 0.4895 - accuracy: 0.7649 - val_loss: 3.7362 - val_accuracy: 0.6139\n",
      "Epoch 39/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4610 - accuracy: 0.7732\n",
      "Epoch 00039: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 27s 1s/step - loss: 0.4590 - accuracy: 0.7741 - val_loss: 2.8360 - val_accuracy: 0.6194\n",
      "Epoch 40/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4327 - accuracy: 0.7925\n",
      "Epoch 00040: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 24s 934ms/step - loss: 0.4307 - accuracy: 0.7939 - val_loss: 3.8103 - val_accuracy: 0.6111\n",
      "Epoch 41/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4359 - accuracy: 0.7951\n",
      "Epoch 00041: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 26s 981ms/step - loss: 0.4435 - accuracy: 0.7890 - val_loss: 3.0887 - val_accuracy: 0.6194\n",
      "Epoch 42/50\n",
      "25/26 [===========================>..] - ETA: 1s - loss: 0.4288 - accuracy: 0.7957\n",
      "Epoch 00042: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 28s 1s/step - loss: 0.4309 - accuracy: 0.7946 - val_loss: 3.3732 - val_accuracy: 0.6194\n",
      "Epoch 43/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4359 - accuracy: 0.8041\n",
      "Epoch 00043: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 24s 904ms/step - loss: 0.4439 - accuracy: 0.8020 - val_loss: 5.8412 - val_accuracy: 0.6167\n",
      "Epoch 44/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4381 - accuracy: 0.7957\n",
      "Epoch 00044: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 25s 965ms/step - loss: 0.4381 - accuracy: 0.7939 - val_loss: 3.0005 - val_accuracy: 0.6167\n",
      "Epoch 45/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4393 - accuracy: 0.7880\n",
      "Epoch 00045: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 25s 947ms/step - loss: 0.4414 - accuracy: 0.7871 - val_loss: 1.7083 - val_accuracy: 0.6319\n",
      "Epoch 46/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4289 - accuracy: 0.8041\n",
      "Epoch 00046: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 26s 984ms/step - loss: 0.4262 - accuracy: 0.8051 - val_loss: 1.4245 - val_accuracy: 0.6208\n",
      "Epoch 47/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4793 - accuracy: 0.7771\n",
      "Epoch 00047: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 23s 877ms/step - loss: 0.4813 - accuracy: 0.7760 - val_loss: 1.9407 - val_accuracy: 0.6583\n",
      "Epoch 48/50\n",
      "25/26 [===========================>..] - ETA: 1s - loss: 0.5183 - accuracy: 0.7648\n",
      "Epoch 00048: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5161 - accuracy: 0.7661 - val_loss: 3.2116 - val_accuracy: 0.6458\n",
      "Epoch 49/50\n",
      "25/26 [===========================>..] - ETA: 1s - loss: 0.4816 - accuracy: 0.7713\n",
      "Epoch 00049: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 38s 1s/step - loss: 0.4871 - accuracy: 0.7704 - val_loss: 3.5808 - val_accuracy: 0.6361\n",
      "Epoch 50/50\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.4694 - accuracy: 0.7822\n",
      "Epoch 00050: val_accuracy did not improve from 0.76667\n",
      "26/26 [==============================] - 27s 1s/step - loss: 0.4678 - accuracy: 0.7828 - val_loss: 2.8566 - val_accuracy: 0.6361\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    aug.flow(x_train, y_train, batch_size=BS),\n",
    "    validation_data=(x_test, y_test),\n",
    "    steps_per_epoch=len(x_train) // BS,\n",
    "    epochs=EPOCHS, verbose=1,\n",
    "    callbacks=[checkpoint]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:41:32.829793Z",
     "start_time": "2019-12-11T19:41:32.283651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "#Train and validation accuracy\n",
    "plt.plot(epochs, acc, 'b', label='Training accurarcy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\n",
    "plt.title('Training and Validation accurarcy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "#Train and validation loss\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset/v1.model\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:41:54.029510Z",
     "start_time": "2019-12-11T19:41:53.055425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x2b5e90ceac8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = model.load('best_model_5conv2dense.h5')\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# json_file = open('D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/LabelledRice/rice_disease_classifer.model', 'r')\n",
    "# loaded_model = model.load\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# loaded_model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"best_model_5conv2dense_woutBG.h5\")\n",
    "model_trained = load_model(\"D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/NewDataset/v1.model\")\n",
    "model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:42:10.406761Z",
     "start_time": "2019-12-11T19:42:05.107608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Calculating model accuracy\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 2.8566 - accuracy: 0.6361\n",
      "Test Accuracy: 63.61111402511597\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Calculating model accuracy\")\n",
    "scores = model_trained.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at each type of leaf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:42:14.152938Z",
     "start_time": "2019-12-11T19:42:12.904658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 23ms/sample - loss: 2.5788 - accuracy: 0.5566\n",
      "Test Accuracy: 55.66037893295288\n"
     ]
    }
   ],
   "source": [
    "scores_brownspot = model_trained.evaluate(x_test[y_test[:,0]==1], y_test[y_test[:,0]==1])\n",
    "print(f\"Test Accuracy: {scores_brownspot[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:42:18.690298Z",
     "start_time": "2019-12-11T19:42:16.648724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 27ms/sample - loss: 0.0476 - accuracy: 0.9778\n",
      "Test Accuracy: 97.77777791023254\n"
     ]
    }
   ],
   "source": [
    "scores_healthy = model_trained.evaluate(x_test[y_test[:,1]==1], y_test[y_test[:,1]==1])\n",
    "print(f\"Test Accuracy: {scores_healthy[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:42:22.293304Z",
     "start_time": "2019-12-11T19:42:21.193715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 26ms/sample - loss: 4.9770 - accuracy: 0.5000\n",
      "Test Accuracy: 50.0\n"
     ]
    }
   ],
   "source": [
    "scores_hispa = model_trained.evaluate(x_test[y_test[:,2]==1], y_test[y_test[:,2]==1])\n",
    "print(f\"Test Accuracy: {scores_hispa[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T19:42:26.245883Z",
     "start_time": "2019-12-11T19:42:24.836716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 1s 26ms/sample - loss: 4.1972 - accuracy: 0.5000\n",
      "Test Accuracy: 50.0\n"
     ]
    }
   ],
   "source": [
    "scores_leafblast = model_trained.evaluate(x_test[y_test[:,3]==1], y_test[y_test[:,3]==1])\n",
    "print(f\"Test Accuracy: {scores_leafblast[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "\n",
    "Training all the models discussed with the processed images, the accuracy of each model on a validation set is shown in Table 1. Comparing the different models, the model trained from top to bottom yielded the highest accuracy at 78.2%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \\begin{table}[]\n",
    "\\begin{tabular}{ll} -->\n",
    "\n",
    "| Base model                                      | Accuracy |\n",
    "|----|----|\n",
    "|VGG16 | 58.4\\% | \n",
    "|VGG16 (with first three blocks are frozen) | 72.2\\%  |\n",
    "|VGG19                                           | 72.4\\%   |\n",
    "|XCeption                                        | 72.2\\%   |\n",
    "|ResNet50                                        | 72.2\\%   |\n",
    "|5-layer convolution                             | 78.2\\%  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking further into the best model and how well it predicts each class, an 85.3% accuracy was found for brown spot, 54.4% for hispa, and 62.0% for leaf blast. On the other hand for healthy rice leaves, it was classified accurately 92.2% of the time. From this, discrepancies on prediction power of the model to different classifications can be observed. Except for brown spots, the model only accurately predicts more than half of rice leaves with disease. \n",
    "\n",
    "This can be attributed to the dataset used wherein the progression of the disease on the leaves were at its initial stages, so the symptoms are not that developed yet. While the model suffers from relatively low accuracy because of this, the power to predict disease at an earlier stage would give the farmers the opportunity to treat the diseases even before it spreads. This would save them time and energy that they have limited amount of and allow them to focus on the growth of their crops with less worries. That said, seeing how effective the model is on predicting if healthy rice leaves are actually healthy, it can prevent farmers from unnecessarily providing treatment when the plants are actually healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different classifications of rice leaf diseases were studied and compared with the healthy rice leaf using the convolutional neural network. Using more than 3000 images as training data and different image pre-processing techniques, we were able to identify what type of disease a leaf has with 78.2\\% accuracy. Diving deeper, we can accurately classify a healthy leaf as healthy which would aid the farmers save resources by not treating the already healthy plants.\n",
    "\n",
    "For future studies, we recommend to improve the accuracy of prediction by exploring other network architecture or adding layers or nodes to the current model, scale the project to other diseases for the model to be more general, and, lastly, consult with farmers and consider their input into the pre-processing and deployment of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The authors would like to acknowledge our Machine Learning 2.0 professors, Prof. Christopher Monterola, and Prof. Erika Legara for all their guidance and support for the completion of this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10443973, 0.4192321 , 0.25452194, 0.22180624]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trained.predict(x_test[y_test[:,1]==1][0][np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape('D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/Test/Labelled/LeafBlast/leafblast1.png')\n",
    "hispa_path = glob.glob(main_directory + '/BGRemoved/Healthy/*')\n",
    "#print(hispa_path)\n",
    "#picture = '/LabelledRice/BGRemoved/Hispa/IMG_20190419_161920.jpg'\n",
    "result = []\n",
    "for i in hispa_path:\n",
    "    #an_image3 = convert_image_to_array('D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive'+picture)\n",
    "    an_image3 = convert_image_to_array(i)\n",
    "    an_image3 = an_image3[np.newaxis]\n",
    "    np_image3_list = np.array(an_image3, dtype=np.float16) / 225.0\n",
    "    #print(an_image.shape)\n",
    "    index_labelled = ['BrownSpot', 'Healthy', 'Hispa', 'LeafBlast']\n",
    "    prob = model_trained.predict(np_image3_list)[0]\n",
    "    #print(prob)\n",
    "    #print(index_labelled[np.argmax(model_trained.predict(np_image3_list)[0])])\n",
    "    result.append(index_labelled[np.argmax(model_trained.predict(np_image3_list)[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2348, 256, 256, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        ...,\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333]],\n",
       "\n",
       "       [[1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        ...,\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333]],\n",
       "\n",
       "       [[1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        ...,\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        ...,\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333]],\n",
       "\n",
       "       [[1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        ...,\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333]],\n",
       "\n",
       "       [[1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        ...,\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333],\n",
       "        [1.1333333, 1.1333333, 1.1333333]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an_image = convert_image_to_array('D://Moy Hao Zhen/University of Malaya/Sem 5 - Internship Intel/UMHackathon/archive/Test/Labelled/BrownSpot/brownspot1.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "202.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
